{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.26002264035216127, accuracy: 0.935\n",
      "loss: 0.23175584847689307, accuracy: 0.955\n",
      "loss: 0.20657602615989132, accuracy: 0.96\n",
      "loss: 0.1843345891030072, accuracy: 0.96\n",
      "loss: 0.16483579869388745, accuracy: 0.965\n",
      "loss: 0.14785352787107414, accuracy: 0.975\n",
      "loss: 0.13314676381184287, accuracy: 0.98\n",
      "loss: 0.120472496410095, accuracy: 0.98\n",
      "loss: 0.10959544575288005, accuracy: 0.98\n",
      "loss: 0.10029469154470166, accuracy: 0.98\n",
      "loss: 0.0923676272250257, accuracy: 0.98\n",
      "loss: 0.08563180817207931, accuracy: 0.98\n",
      "loss: 0.07992526725390207, accuracy: 0.98\n",
      "loss: 0.07510579843323105, accuracy: 0.98\n",
      "loss: 0.07104960777384565, accuracy: 0.98\n",
      "loss: 0.06764962928013206, accuracy: 0.98\n",
      "loss: 0.06481371427117141, accuracy: 0.98\n",
      "loss: 0.06246283211620809, accuracy: 0.98\n",
      "loss: 0.06052936691281587, accuracy: 0.98\n",
      "loss: 0.05895555663916232, accuracy: 0.98\n",
      "loss: 0.057692095356545965, accuracy: 0.98\n",
      "loss: 0.056696902190144866, accuracy: 0.98\n",
      "loss: 0.05593405053140042, accuracy: 0.98\n",
      "loss: 0.05537284517260645, accuracy: 0.98\n",
      "loss: 0.05498703238989356, accuracy: 0.98\n",
      "loss: 0.05475412724054324, accuracy: 0.98\n",
      "loss: 0.05465484276889423, accuracy: 0.98\n",
      "loss: 0.05467260690290198, accuracy: 0.98\n",
      "loss: 0.05479315422985386, accuracy: 0.98\n",
      "loss: 0.055004181349289916, accuracy: 0.98\n",
      "loss: 0.05529505598403921, accuracy: 0.98\n",
      "loss: 0.05565657141385452, accuracy: 0.98\n",
      "loss: 0.05608073904488733, accuracy: 0.99\n",
      "loss: 0.056560613029898954, accuracy: 0.99\n",
      "loss: 0.05709014181026248, accuracy: 0.99\n",
      "loss: 0.057664042270846295, accuracy: 0.99\n",
      "loss: 0.05827769289596397, accuracy: 0.99\n",
      "loss: 0.058927042903270346, accuracy: 0.99\n",
      "loss: 0.05960853482721622, accuracy: 0.99\n",
      "loss: 0.060319038437967584, accuracy: 0.99\n",
      "loss: 0.06105579422776088, accuracy: 0.99\n",
      "loss: 0.06181636498525713, accuracy: 0.99\n",
      "loss: 0.06259859421888075, accuracy: 0.99\n",
      "loss: 0.06340057039033682, accuracy: 0.99\n",
      "loss: 0.06422059608622532, accuracy: 0.99\n",
      "loss: 0.06505716139456058, accuracy: 0.99\n",
      "loss: 0.06590892086879617, accuracy: 0.99\n",
      "loss: 0.06677467355857507, accuracy: 0.99\n",
      "loss: 0.06765334566714794, accuracy: 0.99\n",
      "loss: 0.06854397546292805, accuracy: 0.99\n",
      "loss: 0.06944570012922818, accuracy: 0.99\n",
      "loss: 0.0703577442836956, accuracy: 0.99\n",
      "loss: 0.07127940993886464, accuracy: 0.99\n",
      "loss: nan, accuracy: 0.99\n",
      "loss: nan, accuracy: 0.99\n",
      "loss: nan, accuracy: 0.99\n",
      "loss: nan, accuracy: 0.99\n",
      "loss: nan, accuracy: 0.99\n",
      "loss: nan, accuracy: 0.99\n",
      "loss: nan, accuracy: 0.99\n",
      "loss: nan, accuracy: 0.99\n",
      "loss: nan, accuracy: 0.99\n",
      "loss: nan, accuracy: 0.99\n",
      "loss: nan, accuracy: 0.99\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jorda\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\jorda\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in multiply\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "#GD learning_rate = 0.1\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def sigmoid(s):\n",
    "    return 1 / (1 + np.exp(-s))\n",
    "def loss(y, h):\n",
    "    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "def gradient(X, y, w):\n",
    "    return -(y * X) / (1 + np.exp(-y * np.dot(X, w)))\n",
    "\n",
    "X, y = datasets.make_classification( n_samples=200, n_features=2, random_state=333, n_informative=2, n_redundant=0, n_clusters_per_class=1)\n",
    "\n",
    "X_bias = np.append(np.ones((X.shape[0], 1)), X, axis=1)\n",
    "y_g = np.array([[1] if label == 1 else [-1] for label in y])\n",
    "y_l = np.array([[1] if label == 1 else [0] for label in y])\n",
    "w = np.array([[random.uniform(-1, 1)] for _ in range(X.shape[1]+1)])\n",
    "\n",
    "max_iter = 100\n",
    "learning_rate = 0.1\n",
    "threshold = 0.5\n",
    "\n",
    "for _ in range(max_iter):\n",
    "    E_in_w=(1/y_g.shape[0])*np.sum(gradient(X_bias,y_g,w),axis=0)\n",
    "    v=-E_in_w[:,np.newaxis]\n",
    "    w = w + learning_rate*v\n",
    "    probabilities = sigmoid(np.dot(X_bias, w))\n",
    "    predictions = [[1] if p > threshold else [-1] for p in probabilities]\n",
    "    print(\"loss: {}, accuracy: {}\".format(loss(y_l, probabilities), accuracy_score(y_g, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.9120705514464575, accuracy: 0.225\n",
      "loss: 0.8852443672937457, accuracy: 0.26\n",
      "loss: 0.8581347797958603, accuracy: 0.285\n",
      "loss: 0.8307657920117286, accuracy: 0.32\n",
      "loss: 0.8031661362758414, accuracy: 0.35\n",
      "loss: 0.7753697102033388, accuracy: 0.395\n",
      "loss: 0.7474159756221294, accuracy: 0.445\n",
      "loss: 0.7193502916231531, accuracy: 0.49\n",
      "loss: 0.6912241472064771, accuracy: 0.56\n",
      "loss: 0.6630952543419076, accuracy: 0.59\n",
      "loss: 0.6350274597918407, accuracy: 0.64\n",
      "loss: 0.6070904350722425, accuracy: 0.675\n",
      "loss: 0.5793591097214315, accuracy: 0.735\n",
      "loss: 0.551912824489264, accuracy: 0.785\n",
      "loss: 0.5248341982614023, accuracy: 0.84\n",
      "loss: 0.4982077245149334, accuracy: 0.865\n",
      "loss: 0.472118137708916, accuracy: 0.89\n",
      "loss: 0.44664861417254154, accuracy: 0.915\n",
      "loss: 0.42187889234266424, accuracy: 0.93\n",
      "loss: 0.3978834105699167, accuracy: 0.935\n",
      "loss: 0.3747295650957306, accuracy: 0.96\n",
      "loss: 0.35247618545915427, accuracy: 0.965\n",
      "loss: 0.3311723101118234, accuracy: 0.965\n",
      "loss: 0.3108563231124774, accuracy: 0.975\n",
      "loss: 0.29155548596572345, accuracy: 0.98\n",
      "loss: 0.27328586996978016, accuracy: 0.98\n",
      "loss: 0.2560526669607845, accuracy: 0.98\n",
      "loss: 0.23985083290531206, accuracy: 0.98\n",
      "loss: 0.22466600152345798, accuracy: 0.98\n",
      "loss: 0.21047559516675263, accuracy: 0.98\n",
      "loss: 0.19725005759722428, accuracy: 0.98\n",
      "loss: 0.18495413723128407, accuracy: 0.98\n",
      "loss: 0.17354815827618297, accuracy: 0.98\n",
      "loss: 0.16298922914826988, accuracy: 0.98\n",
      "loss: 0.15323235081075331, accuracy: 0.98\n",
      "loss: 0.14423140068157864, accuracy: 0.98\n",
      "loss: 0.1359399794374368, accuracy: 0.98\n",
      "loss: 0.12831211772246084, accuracy: 0.98\n",
      "loss: 0.12130284720003313, accuracy: 0.985\n",
      "loss: 0.11486864560627338, accuracy: 0.985\n",
      "loss: 0.10896776871588135, accuracy: 0.985\n",
      "loss: 0.1035604837611389, accuracy: 0.985\n",
      "loss: 0.09860921922979664, accuracy: 0.985\n",
      "loss: 0.0940786454646194, accuracy: 0.985\n",
      "loss: 0.0899356994056987, accuracy: 0.985\n",
      "loss: 0.08614956540516416, accuracy: 0.99\n",
      "loss: 0.08269162249061367, accuracy: 0.99\n",
      "loss: 0.07953536689158143, accuracy: 0.99\n",
      "loss: 0.07665631716024367, accuracy: 0.99\n",
      "loss: 0.07403190786533131, accuracy: 0.99\n",
      "loss: 0.07164137664265788, accuracy: 0.99\n",
      "loss: 0.06946564835423842, accuracy: 0.99\n",
      "loss: 0.06748721923615474, accuracy: 0.99\n",
      "loss: 0.06569004319136583, accuracy: 0.99\n",
      "loss: 0.06405942179204427, accuracy: 0.99\n",
      "loss: 0.06258189907978347, accuracy: 0.99\n",
      "loss: 0.061245161874315224, accuracy: 0.99\n",
      "loss: 0.06003794600637582, accuracy: 0.99\n",
      "loss: 0.058949948663761215, accuracy: 0.99\n",
      "loss: 0.057971746868854444, accuracy: 0.99\n",
      "loss: 0.057094721980179626, accuracy: 0.99\n",
      "loss: 0.05631099002070575, accuracy: 0.99\n",
      "loss: 0.05561333757406963, accuracy: 0.99\n",
      "loss: 0.05499516295034643, accuracy: 0.99\n",
      "loss: 0.05445042230035649, accuracy: 0.99\n",
      "loss: 0.05397358034764517, accuracy: 0.99\n",
      "loss: 0.053559565406913416, accuracy: 0.99\n",
      "loss: 0.053203728364209474, accuracy: 0.99\n",
      "loss: 0.05290180530557228, accuracy: 0.99\n",
      "loss: 0.05264988349545861, accuracy: 0.99\n",
      "loss: 0.052444370422972765, accuracy: 0.99\n",
      "loss: 0.05228196565172353, accuracy: 0.99\n",
      "loss: 0.05215963522737131, accuracy: 0.99\n",
      "loss: 0.05207458841508938, accuracy: 0.99\n",
      "loss: 0.052024256556889206, accuracy: 0.99\n",
      "loss: 0.05200627385579592, accuracy: 0.99\n",
      "loss: 0.05201845991004557, accuracy: 0.99\n",
      "loss: 0.05205880383570767, accuracy: 0.99\n",
      "loss: 0.052125449830366294, accuracy: 0.99\n",
      "loss: 0.052216684043705185, accuracy: 0.99\n",
      "loss: 0.05233092263304687, accuracy: 0.99\n",
      "loss: 0.052466700893125336, accuracy: 0.99\n",
      "loss: 0.052622663359667005, accuracy: 0.99\n",
      "loss: 0.05279755479576464, accuracy: 0.99\n",
      "loss: 0.0529902119786119, accuracy: 0.99\n",
      "loss: 0.05319955621197692, accuracy: 0.99\n",
      "loss: 0.053424586496887576, accuracy: 0.99\n",
      "loss: 0.05366437329944053, accuracy: 0.99\n",
      "loss: 0.053918052860477496, accuracy: 0.99\n",
      "loss: 0.05418482199715362, accuracy: 0.99\n",
      "loss: 0.054463933351198486, accuracy: 0.99\n",
      "loss: 0.05475469104298778, accuracy: 0.99\n",
      "loss: 0.055056446694443914, accuracy: 0.985\n",
      "loss: 0.05536859578730723, accuracy: 0.985\n",
      "loss: 0.05569057432649848, accuracy: 0.985\n",
      "loss: 0.05602185578116564, accuracy: 0.985\n",
      "loss: 0.056361948278598445, accuracy: 0.985\n",
      "loss: 0.05671039202853322, accuracy: 0.985\n",
      "loss: 0.05706675695748095, accuracy: 0.985\n",
      "loss: 0.05743064053461781, accuracy: 0.985\n"
     ]
    }
   ],
   "source": [
    "#GD learning_rate = 0.05\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def sigmoid(s):\n",
    "    return 1 / (1 + np.exp(-s))\n",
    "def loss(y, h):\n",
    "    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "def gradient(X, y, w):\n",
    "    return -(y * X) / (1 + np.exp(-y * np.dot(X, w)))\n",
    "\n",
    "X, y = datasets.make_classification( n_samples=200, n_features=2, random_state=333, n_informative=2, n_redundant=0, n_clusters_per_class=1)\n",
    "\n",
    "X_bias = np.append(np.ones((X.shape[0], 1)), X, axis=1)\n",
    "y_g = np.array([[1] if label == 1 else [-1] for label in y])\n",
    "y_l = np.array([[1] if label == 1 else [0] for label in y])\n",
    "w = np.array([[random.uniform(-1, 1)] for _ in range(X.shape[1]+1)])\n",
    "\n",
    "max_iter = 100\n",
    "learning_rate = 0.05\n",
    "threshold = 0.5\n",
    "\n",
    "for _ in range(max_iter):\n",
    "    E_in_w=(1/y_g.shape[0])*np.sum(gradient(X_bias,y_g,w),axis=0)\n",
    "    v=-E_in_w[:,np.newaxis]\n",
    "    w = w + learning_rate*v\n",
    "    probabilities = sigmoid(np.dot(X_bias, w))\n",
    "    predictions = [[1] if p > threshold else [-1] for p in probabilities]\n",
    "    print(\"loss: {}, accuracy: {}\".format(loss(y_l, probabilities), accuracy_score(y_g, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.4035106990360343, accuracy: 0.81\n",
      "loss: 0.39928265127728324, accuracy: 0.815\n",
      "loss: 0.39508331722456747, accuracy: 0.82\n",
      "loss: 0.3909129446445576, accuracy: 0.825\n",
      "loss: 0.38677177450266015, accuracy: 0.825\n",
      "loss: 0.38266004081486593, accuracy: 0.83\n",
      "loss: 0.3785779705028079, accuracy: 0.83\n",
      "loss: 0.374525783252436, accuracy: 0.84\n",
      "loss: 0.37050369137671646, accuracy: 0.845\n",
      "loss: 0.36651189968275305, accuracy: 0.85\n",
      "loss: 0.36255060534372674, accuracy: 0.855\n",
      "loss: 0.3586199977760365, accuracy: 0.855\n",
      "loss: 0.3547202585220128, accuracy: 0.865\n",
      "loss: 0.35085156113856614, accuracy: 0.87\n",
      "loss: 0.3470140710921133, accuracy: 0.87\n",
      "loss: 0.3432079456601081, accuracy: 0.87\n",
      "loss: 0.33943333383948726, accuracy: 0.87\n",
      "loss: 0.33569037626231996, accuracy: 0.875\n",
      "loss: 0.33197920511892803, accuracy: 0.88\n",
      "loss: 0.3282999440887221, accuracy: 0.88\n",
      "loss: 0.3246527082789762, accuracy: 0.88\n",
      "loss: 0.32103760417173627, accuracy: 0.88\n",
      "loss: 0.3174547295790357, accuracy: 0.88\n",
      "loss: 0.3139041736065639, accuracy: 0.885\n",
      "loss: 0.31038601662590715, accuracy: 0.885\n",
      "loss: 0.3069003302554556, accuracy: 0.89\n",
      "loss: 0.303447177350044, accuracy: 0.89\n",
      "loss: 0.3000266119993653, accuracy: 0.89\n",
      "loss: 0.296638679535173, accuracy: 0.895\n",
      "loss: 0.293283416547259, accuracy: 0.905\n",
      "loss: 0.28996085090817064, accuracy: 0.915\n",
      "loss: 0.28667100180660404, accuracy: 0.915\n",
      "loss: 0.2834138797893875, accuracy: 0.925\n",
      "loss: 0.2801894868119451, accuracy: 0.93\n",
      "loss: 0.27699781629710785, accuracy: 0.93\n",
      "loss: 0.27383885320211776, accuracy: 0.935\n",
      "loss: 0.2707125740936511, accuracy: 0.935\n",
      "loss: 0.2676189472306653, accuracy: 0.935\n",
      "loss: 0.26455793265485866, accuracy: 0.935\n",
      "loss: 0.261529482288512, accuracy: 0.935\n",
      "loss: 0.2585335400394681, accuracy: 0.935\n",
      "loss: 0.2555700419129888, accuracy: 0.935\n",
      "loss: 0.2526389161302165, accuracy: 0.935\n",
      "loss: 0.2497400832529565, accuracy: 0.94\n",
      "loss: 0.24687345631448437, accuracy: 0.94\n",
      "loss: 0.24403894095607392, accuracy: 0.95\n",
      "loss: 0.2412364355689347, accuracy: 0.95\n",
      "loss: 0.238465831441239, accuracy: 0.95\n",
      "loss: 0.23572701290991632, accuracy: 0.95\n",
      "loss: 0.23301985751688598, accuracy: 0.955\n",
      "loss: 0.23034423616939861, accuracy: 0.955\n",
      "loss: 0.22770001330415382, accuracy: 0.955\n",
      "loss: 0.22508704705486193, accuracy: 0.955\n",
      "loss: 0.22250518942291805, accuracy: 0.955\n",
      "loss: 0.21995428645085868, accuracy: 0.955\n",
      "loss: 0.21743417839827295, accuracy: 0.955\n",
      "loss: 0.21494469991984627, accuracy: 0.955\n",
      "loss: 0.21248568024521602, accuracy: 0.955\n",
      "loss: 0.21005694336032657, accuracy: 0.955\n",
      "loss: 0.20765830818997622, accuracy: 0.96\n",
      "loss: 0.20528958878125486, accuracy: 0.96\n",
      "loss: 0.2029505944875802, accuracy: 0.96\n",
      "loss: 0.2006411301530472, accuracy: 0.96\n",
      "loss: 0.19836099629681414, accuracy: 0.96\n",
      "loss: 0.19610998929725917, accuracy: 0.96\n",
      "loss: 0.1938879015756484, accuracy: 0.96\n",
      "loss: 0.19169452177906884, accuracy: 0.96\n",
      "loss: 0.1895296349623881, accuracy: 0.96\n",
      "loss: 0.18739302276901348, accuracy: 0.96\n",
      "loss: 0.18528446361023504, accuracy: 0.96\n",
      "loss: 0.18320373284294486, accuracy: 0.96\n",
      "loss: 0.18115060294554006, accuracy: 0.96\n",
      "loss: 0.17912484369182366, accuracy: 0.96\n",
      "loss: 0.17712622232273134, accuracy: 0.96\n",
      "loss: 0.17515450371572158, accuracy: 0.965\n",
      "loss: 0.17320945055167822, accuracy: 0.965\n",
      "loss: 0.17129082347918512, accuracy: 0.965\n",
      "loss: 0.16939838127604218, accuracy: 0.965\n",
      "loss: 0.16753188100790517, accuracy: 0.965\n",
      "loss: 0.16569107818393786, accuracy: 0.965\n",
      "loss: 0.16387572690938015, accuracy: 0.965\n",
      "loss: 0.1620855800349409, accuracy: 0.965\n",
      "loss: 0.16032038930293685, accuracy: 0.965\n",
      "loss: 0.1585799054901076, accuracy: 0.965\n",
      "loss: 0.15686387854704453, accuracy: 0.965\n",
      "loss: 0.1551720577341825, accuracy: 0.965\n",
      "loss: 0.15350419175430863, accuracy: 0.965\n",
      "loss: 0.1518600288815536, accuracy: 0.965\n",
      "loss: 0.15023931708683658, accuracy: 0.965\n",
      "loss: 0.14864180415974299, accuracy: 0.965\n",
      "loss: 0.14706723782682152, accuracy: 0.97\n",
      "loss: 0.1455153658662936, accuracy: 0.97\n",
      "loss: 0.14398593621917496, accuracy: 0.975\n",
      "loss: 0.1424786970968139, accuracy: 0.975\n",
      "loss: 0.14099339708485917, accuracy: 0.975\n",
      "loss: 0.1395297852436728, accuracy: 0.975\n",
      "loss: 0.1380876112052105, accuracy: 0.98\n",
      "loss: 0.13666662526639595, accuracy: 0.98\n",
      "loss: 0.1352665784790202, accuracy: 0.98\n",
      "loss: 0.1338872227362008, accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "#GD learning_rate = 0.01\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def sigmoid(s):\n",
    "    return 1 / (1 + np.exp(-s))\n",
    "def loss(y, h):\n",
    "    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "def gradient(X, y, w):\n",
    "    return -(y * X) / (1 + np.exp(-y * np.dot(X, w)))\n",
    "\n",
    "X, y = datasets.make_classification( n_samples=200, n_features=2, random_state=333, n_informative=2, n_redundant=0, n_clusters_per_class=1)\n",
    "\n",
    "X_bias = np.append(np.ones((X.shape[0], 1)), X, axis=1)\n",
    "y_g = np.array([[1] if label == 1 else [-1] for label in y])\n",
    "y_l = np.array([[1] if label == 1 else [0] for label in y])\n",
    "w = np.array([[random.uniform(-1, 1)] for _ in range(X.shape[1]+1)])\n",
    "\n",
    "max_iter = 100\n",
    "learning_rate = 0.01\n",
    "threshold = 0.5\n",
    "\n",
    "for _ in range(max_iter):\n",
    "    E_in_w=(1/y_g.shape[0])*np.sum(gradient(X_bias,y_g,w),axis=0)\n",
    "    v=-E_in_w[:,np.newaxis]\n",
    "    w = w + learning_rate*v\n",
    "    probabilities = sigmoid(np.dot(X_bias, w))\n",
    "    predictions = [[1] if p > threshold else [-1] for p in probabilities]\n",
    "    print(\"loss: {}, accuracy: {}\".format(loss(y_l, probabilities), accuracy_score(y_g, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.0317923478228568, accuracy: 0.27\n",
      "loss: 1.0263510495826416, accuracy: 0.275\n",
      "loss: 1.0204757560236393, accuracy: 0.275\n",
      "loss: 1.0171743199420373, accuracy: 0.275\n",
      "loss: 1.011891517239047, accuracy: 0.275\n",
      "loss: 1.0050683829207858, accuracy: 0.28\n",
      "loss: 1.0012149395253103, accuracy: 0.28\n",
      "loss: 0.9962182779665076, accuracy: 0.285\n",
      "loss: 0.9905755507829529, accuracy: 0.285\n",
      "loss: 0.9864814996395278, accuracy: 0.285\n",
      "loss: 0.9843528021992765, accuracy: 0.285\n",
      "loss: 0.9772729458195559, accuracy: 0.29\n",
      "loss: 0.9737233305935216, accuracy: 0.29\n",
      "loss: 0.9702530461637434, accuracy: 0.29\n",
      "loss: 0.9652826591498593, accuracy: 0.3\n",
      "loss: 0.9575587006389326, accuracy: 0.31\n",
      "loss: 0.9515847945119756, accuracy: 0.315\n",
      "loss: 0.946922068897156, accuracy: 0.315\n",
      "loss: 0.9400936055802771, accuracy: 0.32\n",
      "loss: 0.9353135193031616, accuracy: 0.33\n",
      "loss: 0.9285283304426157, accuracy: 0.335\n",
      "loss: 0.9244281782280597, accuracy: 0.34\n",
      "loss: 0.9214269008960179, accuracy: 0.34\n",
      "loss: 0.9175913851257872, accuracy: 0.34\n",
      "loss: 0.9129940270020697, accuracy: 0.345\n",
      "loss: 0.9064973662448044, accuracy: 0.35\n",
      "loss: 0.9016154356875653, accuracy: 0.35\n",
      "loss: 0.8947545968384581, accuracy: 0.355\n",
      "loss: 0.8930271481243526, accuracy: 0.355\n",
      "loss: 0.8843099737801728, accuracy: 0.365\n",
      "loss: 0.8811387687413645, accuracy: 0.365\n",
      "loss: 0.8771332099850991, accuracy: 0.37\n",
      "loss: 0.8740924377562475, accuracy: 0.37\n",
      "loss: 0.8665697465308295, accuracy: 0.39\n",
      "loss: 0.8630909993778627, accuracy: 0.39\n",
      "loss: 0.858795444461785, accuracy: 0.39\n",
      "loss: 0.8573816305077128, accuracy: 0.39\n",
      "loss: 0.8528339011965245, accuracy: 0.395\n",
      "loss: 0.8464337910900607, accuracy: 0.395\n",
      "loss: 0.8455172617426132, accuracy: 0.395\n",
      "loss: 0.8379630155812688, accuracy: 0.41\n",
      "loss: 0.8315905942455578, accuracy: 0.41\n",
      "loss: 0.8281554453558493, accuracy: 0.425\n",
      "loss: 0.8240589448852188, accuracy: 0.425\n",
      "loss: 0.8177211875026228, accuracy: 0.425\n",
      "loss: 0.81313596897764, accuracy: 0.43\n",
      "loss: 0.808440578341651, accuracy: 0.43\n",
      "loss: 0.8024610196746429, accuracy: 0.435\n",
      "loss: 0.7948809764769379, accuracy: 0.435\n",
      "loss: 0.7869855385710448, accuracy: 0.44\n",
      "loss: 0.7788669641185075, accuracy: 0.44\n",
      "loss: 0.7707349026697793, accuracy: 0.445\n",
      "loss: 0.7668269082985129, accuracy: 0.45\n",
      "loss: 0.7644925542156141, accuracy: 0.46\n",
      "loss: 0.7606704175870217, accuracy: 0.47\n",
      "loss: 0.7546976784888813, accuracy: 0.47\n",
      "loss: 0.7433854596277272, accuracy: 0.475\n",
      "loss: 0.7385339544878856, accuracy: 0.485\n",
      "loss: 0.735915139630638, accuracy: 0.485\n",
      "loss: 0.7291906312465889, accuracy: 0.495\n",
      "loss: 0.7227183890749233, accuracy: 0.505\n",
      "loss: 0.7187862119070338, accuracy: 0.51\n",
      "loss: 0.7144393748182987, accuracy: 0.52\n",
      "loss: 0.7076136219040268, accuracy: 0.52\n",
      "loss: 0.6991411027899557, accuracy: 0.525\n",
      "loss: 0.6914768590721857, accuracy: 0.53\n",
      "loss: 0.6863980836865909, accuracy: 0.535\n",
      "loss: 0.6794645695321523, accuracy: 0.54\n",
      "loss: 0.6751557392622799, accuracy: 0.54\n",
      "loss: 0.6610781042723175, accuracy: 0.57\n",
      "loss: 0.6557966389009379, accuracy: 0.575\n",
      "loss: 0.6494811450714764, accuracy: 0.585\n",
      "loss: 0.6472220482390288, accuracy: 0.59\n",
      "loss: 0.6404780503721406, accuracy: 0.6\n",
      "loss: 0.6380468831478349, accuracy: 0.6\n",
      "loss: 0.6361576861354452, accuracy: 0.605\n",
      "loss: 0.6305875732327704, accuracy: 0.615\n",
      "loss: 0.6245517664574279, accuracy: 0.625\n",
      "loss: 0.6223891280014754, accuracy: 0.635\n",
      "loss: 0.6157330907108493, accuracy: 0.65\n",
      "loss: 0.610355409393101, accuracy: 0.655\n",
      "loss: 0.605796436933445, accuracy: 0.655\n",
      "loss: 0.6000128737497347, accuracy: 0.655\n",
      "loss: 0.5948619318924048, accuracy: 0.655\n",
      "loss: 0.5869654489277473, accuracy: 0.665\n",
      "loss: 0.5822740508136728, accuracy: 0.67\n",
      "loss: 0.5761976169835856, accuracy: 0.67\n",
      "loss: 0.566134070776888, accuracy: 0.685\n",
      "loss: 0.5603725052152317, accuracy: 0.695\n",
      "loss: 0.5571033171884177, accuracy: 0.705\n",
      "loss: 0.5527353382471374, accuracy: 0.705\n",
      "loss: 0.5393734269970789, accuracy: 0.73\n",
      "loss: 0.5358541692001553, accuracy: 0.735\n",
      "loss: 0.530335999967085, accuracy: 0.74\n",
      "loss: 0.5173286518570298, accuracy: 0.755\n",
      "loss: 0.5117343934272857, accuracy: 0.76\n",
      "loss: 0.5066492066810084, accuracy: 0.78\n",
      "loss: 0.5033475086473004, accuracy: 0.785\n",
      "loss: 0.49983651923731054, accuracy: 0.79\n",
      "loss: 0.4989805358340711, accuracy: 0.79\n"
     ]
    }
   ],
   "source": [
    "#SGD learning_rate = 0.01\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def sigmoid(s):\n",
    "    return 1 / (1 + np.exp(-s))\n",
    "def loss(y, h):\n",
    "    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "def gradient(X, y, w):\n",
    "    return -(y * X) / (1 + np.exp(-y * np.dot(X, w)))\n",
    "\n",
    "X, y = datasets.make_classification( n_samples=200, n_features=2, random_state=333, n_informative=2, n_redundant=0, n_clusters_per_class=1)\n",
    "\n",
    "X_bias = np.append(np.ones((X.shape[0], 1)), X, axis=1)\n",
    "y_g = np.array([[1] if label == 1 else [-1] for label in y])\n",
    "y_l = np.array([[1] if label == 1 else [0] for label in y])\n",
    "w = np.array([[random.uniform(-1, 1)] for _ in range(X.shape[1]+1)])\n",
    "\n",
    "max_iter = 100\n",
    "learning_rate = 0.01\n",
    "threshold = 0.5\n",
    "\n",
    "for _ in range(max_iter):\n",
    "    e_in_w=gradient(X_bias,y_g,w)[np.random.permutation(y_g.shape[0])[0],:]\n",
    "    v=-e_in_w[:,np.newaxis]\n",
    "    w = w + learning_rate*v\n",
    "    \n",
    "    probabilities = sigmoid(np.dot(X_bias, w))\n",
    "    predictions = [[1] if p > threshold else [-1] for p in probabilities]\n",
    "    print(\"loss: {}, accuracy: {}\".format(loss(y_l, probabilities), accuracy_score(y_g, predictions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.624244595544178, accuracy: 0.01\n",
      "loss: 1.6057045083643655, accuracy: 0.01\n",
      "loss: 1.585044610826557, accuracy: 0.01\n",
      "loss: 1.5648106369273855, accuracy: 0.01\n",
      "loss: 1.54797825403996, accuracy: 0.01\n",
      "loss: 1.5292955781032982, accuracy: 0.01\n",
      "loss: 1.5148251324931945, accuracy: 0.01\n",
      "loss: 1.4992608640808882, accuracy: 0.01\n",
      "loss: 1.4855796699385286, accuracy: 0.01\n",
      "loss: 1.4659603969084736, accuracy: 0.01\n",
      "loss: 1.4469927093324026, accuracy: 0.01\n",
      "loss: 1.4289846157580175, accuracy: 0.01\n",
      "loss: 1.4119969097146106, accuracy: 0.01\n",
      "loss: 1.3846670561684704, accuracy: 0.01\n",
      "loss: 1.3692806712914802, accuracy: 0.01\n",
      "loss: 1.3477276877742514, accuracy: 0.01\n",
      "loss: 1.3259189461413894, accuracy: 0.01\n",
      "loss: 1.3020670347702634, accuracy: 0.01\n",
      "loss: 1.281058162806213, accuracy: 0.01\n",
      "loss: 1.265701342730708, accuracy: 0.01\n",
      "loss: 1.2487797662400153, accuracy: 0.01\n",
      "loss: 1.2266073633460564, accuracy: 0.01\n",
      "loss: 1.1958149459806455, accuracy: 0.01\n",
      "loss: 1.164554569702295, accuracy: 0.01\n",
      "loss: 1.1421135488093792, accuracy: 0.01\n",
      "loss: 1.1180915650605292, accuracy: 0.015\n",
      "loss: 1.0814847407865813, accuracy: 0.015\n",
      "loss: 1.041794550290055, accuracy: 0.015\n",
      "loss: 1.0118420200507472, accuracy: 0.02\n",
      "loss: 0.9918116771759057, accuracy: 0.02\n",
      "loss: 0.9687954971967634, accuracy: 0.025\n",
      "loss: 0.9243527864488296, accuracy: 0.055\n",
      "loss: 0.9073931734298539, accuracy: 0.06\n",
      "loss: 0.884819498791162, accuracy: 0.07\n",
      "loss: 0.8737585973856844, accuracy: 0.08\n",
      "loss: 0.8505569067464669, accuracy: 0.125\n",
      "loss: 0.8374703629441325, accuracy: 0.115\n",
      "loss: 0.7959020967211743, accuracy: 0.21\n",
      "loss: 0.7761256044805038, accuracy: 0.285\n",
      "loss: 0.7623443149130078, accuracy: 0.33\n",
      "loss: 0.7342156617950589, accuracy: 0.42\n",
      "loss: 0.7166283213592592, accuracy: 0.475\n",
      "loss: 0.6779761466853137, accuracy: 0.585\n",
      "loss: 0.6617863128331485, accuracy: 0.635\n",
      "loss: 0.6176371744796726, accuracy: 0.72\n",
      "loss: 0.5984950472817989, accuracy: 0.765\n",
      "loss: 0.5798457566701165, accuracy: 0.855\n",
      "loss: 0.5394714861872267, accuracy: 0.855\n",
      "loss: 0.5125473556103225, accuracy: 0.88\n",
      "loss: 0.4910354097494267, accuracy: 0.88\n",
      "loss: 0.46220219717413175, accuracy: 0.88\n",
      "loss: 0.42532437955413827, accuracy: 0.905\n",
      "loss: 0.4157400499860944, accuracy: 0.925\n",
      "loss: 0.38133238362286564, accuracy: 0.94\n",
      "loss: 0.36940979376878835, accuracy: 0.94\n",
      "loss: 0.3463703534983969, accuracy: 0.94\n",
      "loss: 0.3186453112965781, accuracy: 0.94\n",
      "loss: 0.2997665478442242, accuracy: 0.94\n",
      "loss: 0.28304370955878183, accuracy: 0.94\n",
      "loss: 0.27709448522326824, accuracy: 0.945\n",
      "loss: 0.25014280910304704, accuracy: 0.945\n",
      "loss: 0.22236715381702318, accuracy: 0.955\n",
      "loss: 0.20561388105440825, accuracy: 0.95\n",
      "loss: 0.19875486020232935, accuracy: 0.95\n",
      "loss: 0.1854488416186678, accuracy: 0.965\n",
      "loss: 0.17584282916705288, accuracy: 0.955\n",
      "loss: 0.16764453773189866, accuracy: 0.955\n",
      "loss: 0.15700721782399019, accuracy: 0.96\n",
      "loss: 0.1546619061865009, accuracy: 0.95\n",
      "loss: 0.14067138529083037, accuracy: 0.96\n",
      "loss: 0.13374346671008058, accuracy: 0.965\n",
      "loss: 0.12599818619538677, accuracy: 0.96\n",
      "loss: 0.11644180500432771, accuracy: 0.965\n",
      "loss: 0.11585318645717521, accuracy: 0.965\n",
      "loss: 0.11605441568308858, accuracy: 0.965\n",
      "loss: 0.11535325896634617, accuracy: 0.965\n",
      "loss: 0.109469458915439, accuracy: 0.965\n",
      "loss: 0.10941923130197485, accuracy: 0.965\n",
      "loss: 0.1071402577747319, accuracy: 0.965\n",
      "loss: 0.10167286781352505, accuracy: 0.965\n",
      "loss: 0.10052008274984, accuracy: 0.965\n",
      "loss: 0.0926408470038689, accuracy: 0.97\n",
      "loss: 0.08941389275446367, accuracy: 0.97\n",
      "loss: 0.08698608812381023, accuracy: 0.97\n",
      "loss: 0.08546243048263481, accuracy: 0.97\n",
      "loss: 0.08529200938466872, accuracy: 0.97\n",
      "loss: 0.08351665903443564, accuracy: 0.97\n",
      "loss: 0.07882785793223607, accuracy: 0.97\n",
      "loss: 0.07892370186627465, accuracy: 0.97\n",
      "loss: 0.07972653880942597, accuracy: 0.97\n",
      "loss: 0.0783029887186079, accuracy: 0.97\n",
      "loss: 0.07784562531147637, accuracy: 0.97\n",
      "loss: 0.07646727854829463, accuracy: 0.97\n",
      "loss: 0.0751508742420806, accuracy: 0.97\n",
      "loss: 0.07343536091526655, accuracy: 0.97\n",
      "loss: 0.0715486363878132, accuracy: 0.975\n",
      "loss: 0.0711778774699266, accuracy: 0.97\n",
      "loss: 0.07178699520510352, accuracy: 0.97\n",
      "loss: 0.06991704611131863, accuracy: 0.975\n",
      "loss: 0.0688928999526002, accuracy: 0.985\n"
     ]
    }
   ],
   "source": [
    "#SGD learning_rate = 0.05\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def sigmoid(s):\n",
    "    return 1 / (1 + np.exp(-s))\n",
    "def loss(y, h):\n",
    "    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "def gradient(X, y, w):\n",
    "    return -(y * X) / (1 + np.exp(-y * np.dot(X, w)))\n",
    "\n",
    "X, y = datasets.make_classification( n_samples=200, n_features=2, random_state=333, n_informative=2, n_redundant=0, n_clusters_per_class=1)\n",
    "\n",
    "X_bias = np.append(np.ones((X.shape[0], 1)), X, axis=1)\n",
    "y_g = np.array([[1] if label == 1 else [-1] for label in y])\n",
    "y_l = np.array([[1] if label == 1 else [0] for label in y])\n",
    "w = np.array([[random.uniform(-1, 1)] for _ in range(X.shape[1]+1)])\n",
    "\n",
    "max_iter = 100\n",
    "learning_rate = 0.05\n",
    "threshold = 0.5\n",
    "\n",
    "for _ in range(max_iter):\n",
    "    e_in_w=gradient(X_bias,y_g,w)[np.random.permutation(y_g.shape[0])[0],:]\n",
    "    v=-e_in_w[:,np.newaxis]\n",
    "    w = w + learning_rate*v\n",
    "    \n",
    "    probabilities = sigmoid(np.dot(X_bias, w))\n",
    "    predictions = [[1] if p > threshold else [-1] for p in probabilities]\n",
    "    print(\"loss: {}, accuracy: {}\".format(loss(y_l, probabilities), accuracy_score(y_g, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.5488553586143903, accuracy: 0.67\n",
      "loss: 0.5181229093363193, accuracy: 0.695\n",
      "loss: 0.455601465965518, accuracy: 0.79\n",
      "loss: 0.3652055139069126, accuracy: 0.815\n",
      "loss: 0.3244014324892755, accuracy: 0.855\n",
      "loss: 0.27136849845285654, accuracy: 0.91\n",
      "loss: 0.26356598156723443, accuracy: 0.91\n",
      "loss: 0.25210040432009195, accuracy: 0.915\n",
      "loss: 0.21731167649593394, accuracy: 0.925\n",
      "loss: 0.20809965011533382, accuracy: 0.925\n",
      "loss: 0.19210276856490452, accuracy: 0.92\n",
      "loss: 0.1564738140614748, accuracy: 0.955\n",
      "loss: 0.1485224144005881, accuracy: 0.95\n",
      "loss: 0.12283468797902485, accuracy: 0.98\n",
      "loss: 0.11150767394878454, accuracy: 0.98\n",
      "loss: 0.10497828567541241, accuracy: 0.985\n",
      "loss: 0.08683419716796024, accuracy: 0.99\n",
      "loss: 0.08494100874542859, accuracy: 0.985\n",
      "loss: 0.0767798672180179, accuracy: 0.98\n",
      "loss: 0.06732712254812219, accuracy: 0.98\n",
      "loss: 0.06224306133179401, accuracy: 0.985\n",
      "loss: 0.06525969147713827, accuracy: 0.985\n",
      "loss: 0.06315436845642243, accuracy: 0.985\n",
      "loss: 0.061311657320287614, accuracy: 0.985\n",
      "loss: 0.06244072864571958, accuracy: 0.985\n",
      "loss: 0.06241599352032961, accuracy: 0.985\n",
      "loss: 0.06378024856195362, accuracy: 0.985\n",
      "loss: 0.0641511860701108, accuracy: 0.985\n",
      "loss: 0.06111166376554486, accuracy: 0.985\n",
      "loss: 0.06193041459909465, accuracy: 0.985\n",
      "loss: 0.06456589160349868, accuracy: 0.98\n",
      "loss: 0.0621281580586626, accuracy: 0.985\n",
      "loss: 0.06082456732572832, accuracy: 0.985\n",
      "loss: 0.06449820447487792, accuracy: 0.985\n",
      "loss: 0.06222860482055415, accuracy: 0.985\n",
      "loss: 0.0626347806720092, accuracy: 0.985\n",
      "loss: 0.060753334172085845, accuracy: 0.985\n",
      "loss: 0.060158106164092506, accuracy: 0.985\n",
      "loss: 0.061213970816189095, accuracy: 0.985\n",
      "loss: 0.06419089115911812, accuracy: 0.985\n",
      "loss: 0.06328967729395364, accuracy: 0.985\n",
      "loss: 0.0651185848642956, accuracy: 0.985\n",
      "loss: 0.06526659827009583, accuracy: 0.985\n",
      "loss: 0.06675337176882523, accuracy: 0.985\n",
      "loss: 0.06656121433437813, accuracy: 0.985\n",
      "loss: 0.06808013506950651, accuracy: 0.985\n",
      "loss: 0.06894092844750568, accuracy: 0.985\n",
      "loss: 0.06860930777480893, accuracy: 0.985\n",
      "loss: 0.06863025145975393, accuracy: 0.985\n",
      "loss: 0.07210667094419648, accuracy: 0.985\n",
      "loss: 0.07414992634461261, accuracy: 0.985\n",
      "loss: 0.0726828008331161, accuracy: 0.985\n",
      "loss: 0.0728838513375314, accuracy: 0.985\n",
      "loss: 0.07365994102271618, accuracy: 0.985\n",
      "loss: 0.07608335509846481, accuracy: 0.985\n",
      "loss: 0.0804873134504089, accuracy: 0.985\n",
      "loss: 0.08156989870472636, accuracy: 0.985\n",
      "loss: 0.08200934587241564, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.985\n",
      "loss: nan, accuracy: 0.98\n",
      "loss: nan, accuracy: 0.98\n",
      "loss: nan, accuracy: 0.98\n",
      "loss: nan, accuracy: 0.98\n",
      "loss: nan, accuracy: 0.98\n",
      "loss: nan, accuracy: 0.98\n",
      "loss: nan, accuracy: 0.98\n",
      "loss: nan, accuracy: 0.98\n",
      "loss: nan, accuracy: 0.98\n",
      "loss: nan, accuracy: 0.98\n",
      "loss: nan, accuracy: 0.98\n",
      "loss: nan, accuracy: 0.98\n",
      "loss: nan, accuracy: 0.98\n",
      "loss: nan, accuracy: 0.98\n",
      "loss: nan, accuracy: 0.98\n",
      "loss: nan, accuracy: 0.98\n",
      "loss: nan, accuracy: 0.98\n",
      "loss: nan, accuracy: 0.975\n",
      "loss: nan, accuracy: 0.975\n",
      "loss: nan, accuracy: 0.975\n",
      "loss: nan, accuracy: 0.98\n",
      "loss: nan, accuracy: 0.98\n",
      "loss: nan, accuracy: 0.98\n",
      "loss: nan, accuracy: 0.975\n",
      "loss: nan, accuracy: 0.975\n",
      "loss: nan, accuracy: 0.98\n",
      "loss: nan, accuracy: 0.98\n",
      "loss: nan, accuracy: 0.975\n",
      "loss: nan, accuracy: 0.975\n",
      "loss: nan, accuracy: 0.975\n",
      "loss: nan, accuracy: 0.98\n",
      "loss: nan, accuracy: 0.975\n",
      "loss: nan, accuracy: 0.98\n",
      "loss: nan, accuracy: 0.98\n",
      "loss: nan, accuracy: 0.98\n",
      "loss: nan, accuracy: 0.98\n",
      "loss: nan, accuracy: 0.98\n",
      "loss: nan, accuracy: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jorda\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\jorda\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: RuntimeWarning: invalid value encountered in multiply\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "#SGD learning_rate = 0.1\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def sigmoid(s):\n",
    "    return 1 / (1 + np.exp(-s))\n",
    "def loss(y, h):\n",
    "    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "def gradient(X, y, w):\n",
    "    return -(y * X) / (1 + np.exp(-y * np.dot(X, w)))\n",
    "\n",
    "X, y = datasets.make_classification( n_samples=200, n_features=2, random_state=333, n_informative=2, n_redundant=0, n_clusters_per_class=1)\n",
    "\n",
    "X_bias = np.append(np.ones((X.shape[0], 1)), X, axis=1)\n",
    "y_g = np.array([[1] if label == 1 else [-1] for label in y])\n",
    "y_l = np.array([[1] if label == 1 else [0] for label in y])\n",
    "w = np.array([[random.uniform(-1, 1)] for _ in range(X.shape[1]+1)])\n",
    "\n",
    "max_iter = 100\n",
    "learning_rate = 0.1\n",
    "threshold = 0.5\n",
    "\n",
    "for _ in range(max_iter):\n",
    "    e_in_w=gradient(X_bias,y_g,w)[np.random.permutation(y_g.shape[0])[0],:]\n",
    "    v=-e_in_w[:,np.newaxis]\n",
    "    w = w + learning_rate*v\n",
    "    \n",
    "    probabilities = sigmoid(np.dot(X_bias, w))\n",
    "    predictions = [[1] if p > threshold else [-1] for p in probabilities]\n",
    "    print(\"loss: {}, accuracy: {}\".format(loss(y_l, probabilities), accuracy_score(y_g, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
